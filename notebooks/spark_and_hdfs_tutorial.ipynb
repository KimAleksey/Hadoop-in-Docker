{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üêòüéì Spark & HDFS: –ü–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ\n",
                "\n",
                "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–∑–¥–∞–Ω, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –≤–∞—Å —Ä–∞–±–æ—Ç–∞—Ç—å —Å **HDFS** –∏ **Apache Spark** –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ.\n",
                "\n",
                "## –ß–µ–º—É –º—ã –Ω–∞—É—á–∏–º—Å—è:\n",
                "1.  –†–∞–±–æ—Ç–∞—Ç—å —Å HDFS —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –ø—Ä—è–º–æ –∏–∑ Jupyter.\n",
                "2.  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å SparkSession.\n",
                "3.  –ß–∏—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ HDFS (Parquet).\n",
                "4.  –í—ã–ø–æ–ª–Ω—è—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö (Filter, GroupBy, Agg).\n",
                "5.  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SQL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n",
                "6.  –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ HDFS."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –®–∞–≥ 1: –†–∞–±–æ—Ç–∞ —Å HDFS (–ö–æ–Ω—Å–æ–ª—å)\n",
                "\n",
                "–í Jupyter –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å bash-–∫–æ–º–∞–Ω–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—è –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫ `!`.\n",
                "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ HDFS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ü—Ä–æ–≤–µ—Ä–∏–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é HDFS\n",
                "!hdfs dfs -ls /"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–∞—à—É –ø–∞–ø–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –≥–¥–µ –ª–µ–∂–∞—Ç –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫—Å–∏\n",
                "!hdfs dfs -ls /user/myself/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –®–∞–≥ 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark\n",
                "\n",
                "–ß—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞–Ω–Ω—ã–º–∏, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Ç–æ—á–∫—É –≤—Ö–æ–¥–∞ ‚Äî `SparkSession`.\n",
                "–ú—ã —É–∫–∞–∑—ã–≤–∞–µ–º `master`, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—à–µ–º—É `spark-master` —Å–µ—Ä–≤–∏—Å—É –≤ Docker."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, month, year, count, avg, desc\n",
                "\n",
                "# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"SparkHDFSTutorial\") \\\n",
                "    .master(\"spark://spark-master:7077\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(f\"Spark Version: {spark.version}\")\n",
                "spark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –®–∞–≥ 3: –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (Data Loading)\n",
                "\n",
                "Spark —É–º–µ–µ—Ç —á–∏—Ç–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤: CSV, JSON, Parquet, Avro, ORC.\n",
                "–í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ **Parquet** (–∫–æ–ª–æ–Ω–æ—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –∏–∑ HDFS\n",
                "file_path = \"hdfs://namenode:9000/user/myself/yellow_tripdata_2023-01.parquet\"\n",
                "df = spark.read.parquet(file_path)\n",
                "\n",
                "# –í—ã–≤–µ–¥–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö (—Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫)\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –®–∞–≥ 4: –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å DataFrame\n",
                "\n",
                "DataFrame API –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –í—ã–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (Select)\n",
                "df.select(\"tpep_pickup_datetime\", \"passenger_count\", \"total_amount\").show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (Filter / Where)\n",
                "# –ù–∞–π–¥–µ–º –ø–æ–µ–∑–¥–∫–∏, –≥–¥–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å –±–æ–ª—å—à–µ $100\n",
                "expensive_trips = df.filter(col(\"total_amount\") > 100)\n",
                "\n",
                "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Ä–æ–≥–∏—Ö –ø–æ–µ–∑–¥–æ–∫: {expensive_trips.count()}\")\n",
                "expensive_trips.select(\"tpep_pickup_datetime\", \"total_amount\").show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è (GroupBy)\n",
                "# –ü–æ—Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ VendorID\n",
                "df.groupBy(\"VendorID\") \\\n",
                "  .agg(\n",
                "      count(\"*\").alias(\"total_trips\"),\n",
                "      avg(\"total_amount\").alias(\"avg_fare\")\n",
                "  ) \\\n",
                "  .show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –®–∞–≥ 5: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ SQL\n",
                "\n",
                "–ï—Å–ª–∏ –≤—ã –∑–Ω–∞–µ—Ç–µ SQL, Spark –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é. –î–ª—è —ç—Ç–æ–≥–æ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.createOrReplaceTempView(\"taxi_trips\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ü—Ä–∏–º–µ—Ä SQL-–∑–∞–ø—Ä–æ—Å–∞: –¢–æ–ø 5 —Å–∞–º—ã—Ö –¥–æ—Ä–æ–≥–∏—Ö –ø–æ–µ–∑–¥–æ–∫\n",
                "spark.sql(\"\"\"\n",
                "    SELECT \n",
                "        tpep_pickup_datetime, \n",
                "        passenger_count, \n",
                "        trip_distance, \n",
                "        total_amount\n",
                "    FROM taxi_trips\n",
                "    ORDER BY total_amount DESC\n",
                "    LIMIT 5\n",
                "\"\"\").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –®–∞–≥ 6: –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ HDFS\n",
                "\n",
                "–î–æ–ø—É—Å—Ç–∏–º, –º—ã —Å–¥–µ–ª–∞–ª–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—é –∏ —Ö–æ—Ç–∏–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –º–∞–ª–µ–Ω—å–∫–∏–π DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
                "summary_df = df.groupBy(\"payment_type\").count()\n",
                "\n",
                "# –°–æ—Ö—Ä–∞–Ω–∏–º –µ–≥–æ –≤ CSV –≤ HDFS\n",
                "output_path = \"hdfs://namenode:9000/user/myself/payment_summary\"\n",
                "\n",
                "# mode(\"overwrite\") –ø–µ—Ä–µ–∑–∞–ø–∏—à–µ—Ç –ø–∞–ø–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
                "summary_df.write.mode(\"overwrite\").csv(output_path)\n",
                "\n",
                "print(\"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –ü—Ä–æ–≤–µ—Ä–∏–º —á–µ—Ä–µ–∑ HDFS CLI, —á—Ç–æ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–ª–∏—Å—å\n",
                "!hdfs dfs -ls /user/myself/payment_summary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ –í—ã–≤–æ–¥\n",
                "\n",
                "–í—ã –ø—Ä–æ—à–ª–∏ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã Data Engineer'–∞:\n",
                "1.  –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (HDFS).\n",
                "2.  –ó–∞–≥—Ä—É–∑–∏–ª–∏ –∏—Ö –≤ –¥–≤–∏–∂–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (Spark).\n",
                "3.  –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ (SQL/DataFrame).\n",
                "4.  –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (ETL).\n",
                "\n",
                "–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å–≤–æ–∏ —Ñ–∞–π–ª—ã –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}