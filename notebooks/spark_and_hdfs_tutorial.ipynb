{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêòüéì Spark & HDFS: –ü–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ\n",
    "\n",
    "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–∑–¥–∞–Ω, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –≤–∞—Å —Ä–∞–±–æ—Ç–∞—Ç—å —Å **HDFS** –∏ **Apache Spark** –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ.\n",
    "\n",
    "## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "–í—ã–ø–æ–ª–Ω–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–º–∞–Ω–¥—ã Hadoop –≤ —Å–∏—Å—Ç–µ–º—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_HOME=/home/jovyan/work/hadoop\n",
      "env: HADOOP_CONF_DIR=/home/jovyan/work/hadoop/etc/hadoop\n",
      "env: JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64\n",
      "env: PATH=/home/jovyan/work/hadoop/bin:/home/jovyan/work/hadoop/bin:/home/jovyan/work/hadoop/bin:/home/jovyan/work/hadoop/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/spark/bin\n",
      "‚úÖ Environment congigured!\n",
      "   HADOOP_HOME: /home/jovyan/work/hadoop\n",
      "   JAVA_HOME:   /usr/lib/jvm/java-17-openjdk-arm64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# –ü—É—Ç—å –∫ Hadoop\n",
    "hadoop_home = \"/home/jovyan/work/hadoop\"\n",
    "hadoop_bin = hadoop_home + \"/bin\"\n",
    "hadoop_conf = hadoop_home + \"/etc/hadoop\"\n",
    "\n",
    "# –ü—É—Ç—å –∫ Java (–Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)\n",
    "java_home = \"/usr/lib/jvm/java-17-openjdk-arm64\"\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "# %env - —ç—Ç–æ –º–∞–≥–∏—è IPython, –∫–æ—Ç–æ—Ä–∞—è —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ (!)\n",
    "%env HADOOP_HOME={hadoop_home}\n",
    "%env HADOOP_CONF_DIR={hadoop_conf}\n",
    "%env JAVA_HOME={java_home}\n",
    "%env PATH={hadoop_bin}:{os.environ['PATH']}\n",
    "\n",
    "print(f\"‚úÖ Environment congigured!\")\n",
    "print(f\"   HADOOP_HOME: {hadoop_home}\")\n",
    "print(f\"   JAVA_HOME:   {java_home}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –∫–æ–º–∞–Ω–¥–∞ `hdfs` —Ä–∞–±–æ—Ç–∞–µ—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 3.3.6\n",
      "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
      "Compiled by ubuntu on 2023-06-18T08:22Z\n",
      "Compiled on platform linux-x86_64\n",
      "Compiled with protoc 3.7.1\n",
      "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
      "This command was run using /home/jovyan/work/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n"
     ]
    }
   ],
   "source": [
    "!hdfs version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è –ï—Å–ª–∏ —è—á–µ–π–∫–∞ –≤—ã—à–µ –∑–∞–≤–∏—Å–ª–∞ —Å–æ –∑–Ω–∞–∫–æ–º `[*]`:\n",
    "–≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ Jupyter –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ Java. –û–±—ã—á–Ω–æ —è—á–µ–π–∫–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞ 1-2 —Å–µ–∫—É–Ω–¥—ã.\n",
    "–ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ, –Ω–∞–∂–º–∏—Ç–µ **Kernel -> Restart Kernel** –≤ –º–µ–Ω—é —Å–≤–µ—Ä—Ö—É –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 1: –†–∞–±–æ—Ç–∞ —Å HDFS (–ö–æ–Ω—Å–æ–ª—å)\n",
    "\n",
    "–í Jupyter –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å bash-–∫–æ–º–∞–Ω–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—è –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫ `!`.\n",
    "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-08 20:54:16,546 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2026-02-08 19:45 /test_dir\n",
      "drwxr-xr-x   - root supergroup          0 2026-02-08 20:03 /user\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-08 20:54:19,696 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup   47673370 2026-02-08 20:00 /user/myself/yellow_tripdata_2023-01.parquet\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–∞—à—É –ø–∞–ø–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "!hdfs dfs -ls /user/myself/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark\n",
    "\n",
    "–°–æ–∑–¥–∞–µ–º `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e6d0357bcfb3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkHDFSTutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff943f8390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, desc\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkHDFSTutorial\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 3: –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (Data Loading)\n",
    "\n",
    "–ß–∏—Ç–∞–µ–º Parquet —Ñ–∞–π–ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –∏–∑ HDFS\n",
    "file_path = \"hdfs://namenode:9000/user/myself/yellow_tripdata_2023-01.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# –í—ã–≤–µ–¥–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö (—Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 4: –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------------+\n",
      "|tpep_pickup_datetime|passenger_count|total_amount|\n",
      "+--------------------+---------------+------------+\n",
      "| 2023-01-01 00:32:10|            1.0|        14.3|\n",
      "| 2023-01-01 00:55:08|            1.0|        16.9|\n",
      "| 2023-01-01 00:25:04|            1.0|        34.9|\n",
      "| 2023-01-01 00:03:48|            0.0|       20.85|\n",
      "| 2023-01-01 00:10:29|            1.0|       19.68|\n",
      "+--------------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –í—ã–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (Select)\n",
    "df.select(\"tpep_pickup_datetime\", \"passenger_count\", \"total_amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Ä–æ–≥–∏—Ö –ø–æ–µ–∑–¥–æ–∫: 36001\n",
      "+--------------------+------------+\n",
      "|tpep_pickup_datetime|total_amount|\n",
      "+--------------------+------------+\n",
      "| 2023-01-01 00:29:33|      107.87|\n",
      "| 2023-01-01 00:51:35|      103.55|\n",
      "| 2023-01-01 00:19:10|      189.98|\n",
      "| 2023-01-01 00:31:05|      100.33|\n",
      "| 2023-01-01 00:31:00|      115.37|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (Filter / Where)\n",
    "# –ù–∞–π–¥–µ–º –ø–æ–µ–∑–¥–∫–∏, –≥–¥–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å –±–æ–ª—å—à–µ $100\n",
    "expensive_trips = df.filter(col(\"total_amount\") > 100)\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Ä–æ–≥–∏—Ö –ø–æ–µ–∑–¥–æ–∫: {expensive_trips.count()}\")\n",
    "expensive_trips.select(\"tpep_pickup_datetime\", \"total_amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+\n",
      "|VendorID|total_trips|          avg_fare|\n",
      "+--------+-----------+------------------+\n",
      "|       1|     827367|25.755021350872866|\n",
      "|       2|    2239399| 27.48788289622012|\n",
      "+--------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –ê–≥—Ä–µ–≥–∞—Ü–∏—è (GroupBy)\n",
    "# –ü–æ—Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ VendorID\n",
    "df.groupBy(\"VendorID\") \\\n",
    "  .agg(\n",
    "      count(\"*\").alias(\"total_trips\"),\n",
    "      avg(\"total_amount\").alias(\"avg_fare\")\n",
    "  ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 5: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"taxi_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+------------+\n",
      "|tpep_pickup_datetime|passenger_count|trip_distance|total_amount|\n",
      "+--------------------+---------------+-------------+------------+\n",
      "| 2023-01-24 12:43:44|            1.0|       177.88|      1169.4|\n",
      "| 2023-01-09 16:17:32|            1.0|          0.0|      1000.0|\n",
      "| 2023-01-30 13:17:33|            1.0|          0.0|       901.0|\n",
      "| 2023-01-30 13:23:56|            1.0|          0.0|       751.0|\n",
      "| 2023-01-30 16:17:35|            2.0|         8.81|       705.6|\n",
      "+--------------------+---------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä SQL-–∑–∞–ø—Ä–æ—Å–∞: –¢–æ–ø 5 —Å–∞–º—ã—Ö –¥–æ—Ä–æ–≥–∏—Ö –ø–æ–µ–∑–¥–æ–∫\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        tpep_pickup_datetime, \n",
    "        passenger_count, \n",
    "        trip_distance, \n",
    "        total_amount\n",
    "    FROM taxi_trips\n",
    "    ORDER BY total_amount DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –®–∞–≥ 6: –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –º–∞–ª–µ–Ω—å–∫–∏–π DataFrame\n",
    "summary_df = df.groupBy(\"payment_type\").count()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–∏–º –µ–≥–æ –≤ CSV –≤ HDFS\n",
    "output_path = \"hdfs://namenode:9000/user/myself/payment_summary\"\n",
    "summary_df.write.mode(\"overwrite\").csv(output_path)\n",
    "\n",
    "print(\"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-08 20:58:55,578 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 jovyan supergroup          0 2026-02-08 20:58 /user/myself/payment_summary/_SUCCESS\n",
      "-rw-r--r--   3 jovyan supergroup         43 2026-02-08 20:58 /user/myself/payment_summary/part-00000-8f63b1da-d9e2-40a4-abaf-aa5f0ec92821-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º\n",
    "!hdfs dfs -ls /user/myself/payment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
