# Hadoop & Spark Docker Sandbox ðŸ˜âš¡ï¸

ÐŸÑ€Ð¾ÐµÐºÑ‚ Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ñ ÑÐºÐ¾ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹ Hadoop (HDFS, YARN, MapReduce) Ð¸ Apache Spark Ð² Docker-ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°Ñ….

## ðŸ’» Ð§Ñ‚Ð¾ Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ
Ð’ Ð´Ð°Ð½Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð° Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ð°Ñ ÑÑ€ÐµÐ´Ð° Big Data. ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹:
- âœ”ï¸ **Docker Compose** - Ð´Ð»Ñ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð².
- âœ”ï¸ **Hadoop HDFS** - Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð°Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° (NameNode + DataNode).
- âœ”ï¸ **Hadoop YARN** - Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² (ResourceManager + NodeManager).
- âœ”ï¸ **Hadoop MapReduce** - ÐºÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹ (HistoryServer).
- âœ”ï¸ **Apache Spark** - ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð²Ð¸Ð¶Ð¾Ðº Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (Master + Worker).
- âœ”ï¸ **Jupyter / Shell** - Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾.

## ðŸ“ Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
```
Hadoop/
â”œâ”€â”€ docker-compose.yml   # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
â”œâ”€â”€ learning_plan.md     # ÐŸÐ¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (HDFS -> MapReduce -> Spark)
â”œâ”€â”€ notebooks/           # Jupyter Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐºÐ¸
â”‚   â”œâ”€â”€ nyc_taxi_analysis.ipynb
â”‚   â””â”€â”€ spark_and_hdfs_tutorial.ipynb
â””â”€â”€ README.md            # Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° (ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð»)
```

## ðŸš€ Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¸ Ð—Ð°Ð¿ÑƒÑÐº

### ÐŸÑ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ
- Docker Desktop (Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Linux ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð¾Ð²)
- 4GB+ RAM Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¾ Ð¿Ð¾Ð´ Docker

### Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð² Ð¿Ð°Ð¿ÐºÑƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ:
```bash
docker-compose up -d
```

ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð¾Ð²:
```bash
docker-compose ps
```

### Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ°Ð¼
ÐŸÐ¾ÑÐ»Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ Web UI:
- **HDFS NameNode**: [http://localhost:9870](http://localhost:9870)
- **YARN ResourceManager**: [http://localhost:8088](http://localhost:8088)
- **Spark Master**: [http://localhost:8090](http://localhost:8090)
- **MapReduce History Server**: [http://localhost:8188](http://localhost:8188)
- **Jupyter Notebook**: [http://localhost:8888](http://localhost:8888) (Ð¿Ð°Ñ€Ð¾Ð»ÑŒ/Ñ‚Ð¾ÐºÐµÐ½ ÑÐ¼. Ð² Ð»Ð¾Ð³Ð°Ñ… `docker-compose logs jupyter`)

## ðŸ§‘â€ðŸ’» Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð¼

### 1. Jupyter Notebook (PySpark)
Ð¡Ð°Ð¼Ñ‹Ð¹ ÑƒÐ´Ð¾Ð±Ð½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸.

**Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ ÑÑ‚Ð°Ñ€Ñ‚ Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼:**
1. ÐžÑ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ [http://localhost:8888](http://localhost:8888).
2. ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð² Ð¿Ð°Ð¿ÐºÑƒ `work`.
3. ÐžÑ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ Ñ„Ð°Ð¹Ð» **`spark_and_hdfs_tutorial.ipynb`**.
4. Ð­Ñ‚Ð¾ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑƒÑ€Ð¾Ðº: Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ð¹Ñ‚Ðµ ÐºÐ¾Ð´ ÑÑ‡ÐµÐ¹ÐºÐ° Ð·Ð° ÑÑ‡ÐµÐ¹ÐºÐ¾Ð¹ (Shift+Enter).

**ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ:**
1. Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ð½Ð¾Ð²Ñ‹Ð¹ Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐº.
2. Ð’ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ ÐºÐ¾Ð´:

```python
from pyspark.sql import SparkSession

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ SparkSession Ñ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ðº ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ñƒ
spark = SparkSession.builder \
    .appName("JupyterTest") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ
data = [("Done", 1), ("Success", 2)]
df = spark.createDataFrame(data, ["Status", "ID"])
df.show()

print(f"Spark Version: {spark.version}")
spark.stop()
```

### 2. Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ HDFS (CLI)
Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð°Ð¿ÐºÐ¸ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°:
```bash
# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
docker exec namenode hdfs dfs -mkdir -p /user/myself

# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ðµ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÐµÐ³Ð¾ Ð² HDFS
docker exec namenode bash -c "echo 'Hello Big Data' > /tmp/test.txt"
docker exec namenode hdfs dfs -put /tmp/test.txt /user/myself/

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»
docker exec namenode hdfs dfs -ls /user/myself/
```

### 2. Ð—Ð°Ð¿ÑƒÑÐº MapReduce (WordCount)
ÐšÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ð° ÑÐ»Ð¾Ð² Ð½Ð° Hadoop:
```bash
# ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
docker exec namenode bash -c "echo 'Hadoop Spark Hadoop YARN' > /tmp/input.txt"
docker exec namenode hdfs dfs -mkdir -p /user/wordcount/input
docker exec namenode hdfs dfs -put /tmp/input.txt /user/wordcount/input/

# Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ð¼ Ð·Ð°Ð´Ð°Ñ‡Ñƒ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð²ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ñ‹Ð¹ JAR)
docker exec resourcemanager yarn jar \
  /opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar \
  wordcount /user/wordcount/input /user/wordcount/output

# ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
docker exec namenode hdfs dfs -cat /user/wordcount/output/part-r-00000
```

### 3. Ð Ð°Ð±Ð¾Ñ‚Ð° ÑÐ¾ Spark (Scala Shell)
Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð°Ñ ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ Ð´Ð»Ñ Data Science Ð·Ð°Ð´Ð°Ñ‡:
```bash
docker exec -it spark-master /opt/spark/bin/spark-shell --master spark://spark-master:7077
```

Ð’Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸ (Ñ€Ð°ÑÑ‡ÐµÑ‚ Ñ‡Ð¸ÑÐ»Ð° ÐŸÐ¸):
```scala
val count = sc.parallelize(1 to 100000).filter { _ =>
  val x = math.random
  val y = math.random
  x*x + y*y < 1
}.count()
println(s"Pi is roughly ${4.0 * count / 100000}")
```

### 4. Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ (ÐŸÑ€Ð¸Ð¼ÐµÑ€ NYC Taxi)
Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° (Parquet) Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ‡ÐµÑ€ÐµÐ· Spark SQL:

**Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…:**
```bash
# Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð» Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€
docker exec namenode curl -o /tmp/taxi.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet

# ÐšÐ»Ð°Ð´ÐµÐ¼ Ð² HDFS
docker exec namenode hdfs dfs -put /tmp/taxi.parquet /user/myself/
```

**ÐÐ½Ð°Ð»Ð¸Ð· (Ð² Spark Shell):**
```scala
val df = spark.read.parquet("hdfs://namenode:9000/user/myself/taxi.parquet")
df.createOrReplaceTempView("taxi")
spark.sql("SELECT passenger_count, count(*) FROM taxi GROUP BY 1").show()
```

## ðŸ“– ÐŸÐ»Ð°Ð½ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
Ð’ Ñ„Ð°Ð¹Ð»Ðµ `learning_plan.md` Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð¾Ð°Ð´Ð¼Ð°Ð¿ Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¹ Ð² ÑÑ‚Ð¾Ð¼ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ð¸.

## ðŸ”§ Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ (Troubleshooting)

### 1. Jupyter: `hdfs: command not found`
Ð•ÑÐ»Ð¸ Ð² Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐºÐµ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ `!hdfs`, ÑƒÐ±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð° ÑÑ‡ÐµÐ¹ÐºÐ° Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐºÐ°.
ÐžÐ½Ð° Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ð¿ÑƒÑ‚Ð¸ Ðº Ð±Ð¸Ð½Ð°Ñ€Ð½Ð¸ÐºÐ°Ð¼ Hadoop (ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð² `notebooks/hadoop`).

### 2. Spark SQL: `TABLE_OR_VIEW_NOT_FOUND`
Ð•ÑÐ»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ `SELECT ... FROM table` Ð¿Ð°Ð´Ð°ÐµÑ‚ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹, Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð²Ñ‹ Ð·Ð°Ð±Ñ‹Ð»Ð¸ Ð·Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ DataFrame ÐºÐ°Ðº Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ.
Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ Ð¿ÐµÑ€ÐµÐ´ SQL Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð¼:
```python
df.createOrReplaceTempView("table_name")
```

### 3. HDFS: `Permission denied`
Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð·Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»Ñ‹ Ð² HDFS Ð¸Ð· Jupyter (Ð¾ÑˆÐ¸Ð±ÐºÐ° `AccessControlException`), Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ñƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ `jovyan` Ð½ÐµÑ‚ Ð¿Ñ€Ð°Ð² Ð½Ð° Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² Ð¿Ð°Ð¿ÐºÑƒ.
Ð ÐµÑˆÐµÐ½Ð¸Ðµ (Ñ€Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð²ÑÐµÐ¼):
```bash
docker exec namenode hdfs dfs -chmod 777 /user/myself
```
