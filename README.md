# Hadoop & Spark Docker Sandbox ðŸ˜âš¡ï¸

ÐŸÑ€Ð¾ÐµÐºÑ‚ Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ñ ÑÐºÐ¾ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹ Hadoop (HDFS, YARN, MapReduce) Ð¸ Apache Spark Ð² Docker-ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°Ñ….

## ðŸ’» Ð§Ñ‚Ð¾ Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ
Ð’ Ð´Ð°Ð½Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð° Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ð°Ñ ÑÑ€ÐµÐ´Ð° Big Data. ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹:
- âœ”ï¸ **Docker Compose** - Ð´Ð»Ñ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð².
- âœ”ï¸ **Hadoop HDFS** - Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð°Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° (NameNode + DataNode).
- âœ”ï¸ **Hadoop YARN** - Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² (ResourceManager + NodeManager).
- âœ”ï¸ **Hadoop MapReduce** - ÐºÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹ (HistoryServer).
- âœ”ï¸ **Apache Spark** - ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð²Ð¸Ð¶Ð¾Ðº Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (Master + Worker).
- âœ”ï¸ **Jupyter / Shell** - Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾.

## ðŸ“ Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
```
Hadoop/
â”œâ”€â”€ docker-compose.yml   # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
â”œâ”€â”€ learning_plan.md     # ÐŸÐ¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (HDFS -> MapReduce -> Spark)
â””â”€â”€ README.md            # Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° (ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð»)
```

## ðŸš€ Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¸ Ð—Ð°Ð¿ÑƒÑÐº

### ÐŸÑ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ
- Docker Desktop (Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Linux ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð¾Ð²)
- 4GB+ RAM Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¾ Ð¿Ð¾Ð´ Docker

### Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð² Ð¿Ð°Ð¿ÐºÑƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ:
```bash
docker-compose up -d
```

ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð¾Ð²:
```bash
docker-compose ps
```

### Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ°Ð¼
ÐŸÐ¾ÑÐ»Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ Web UI:
- **HDFS NameNode**: [http://localhost:9870](http://localhost:9870)
- **YARN ResourceManager**: [http://localhost:8088](http://localhost:8088)
- **Spark Master**: [http://localhost:8090](http://localhost:8090)
- **MapReduce History Server**: [http://localhost:8188](http://localhost:8188)

## ðŸ§‘â€ðŸ’» Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð¼

### 1. Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ HDFS (CLI)
Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð°Ð¿ÐºÐ¸ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°:
```bash
# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
docker exec namenode hdfs dfs -mkdir -p /user/myself

# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ðµ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÐµÐ³Ð¾ Ð² HDFS
docker exec namenode bash -c "echo 'Hello Big Data' > /tmp/test.txt"
docker exec namenode hdfs dfs -put /tmp/test.txt /user/myself/

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»
docker exec namenode hdfs dfs -ls /user/myself/
```

### 2. Ð—Ð°Ð¿ÑƒÑÐº MapReduce (WordCount)
ÐšÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ð° ÑÐ»Ð¾Ð² Ð½Ð° Hadoop:
```bash
# ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
docker exec namenode bash -c "echo 'Hadoop Spark Hadoop YARN' > /tmp/input.txt"
docker exec namenode hdfs dfs -mkdir -p /user/wordcount/input
docker exec namenode hdfs dfs -put /tmp/input.txt /user/wordcount/input/

# Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ð¼ Ð·Ð°Ð´Ð°Ñ‡Ñƒ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð²ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ñ‹Ð¹ JAR)
docker exec resourcemanager yarn jar \
  /opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar \
  wordcount /user/wordcount/input /user/wordcount/output

# ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
docker exec namenode hdfs dfs -cat /user/wordcount/output/part-r-00000
```

### 3. Ð Ð°Ð±Ð¾Ñ‚Ð° ÑÐ¾ Spark (Scala Shell)
Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð°Ñ ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ Ð´Ð»Ñ Data Science Ð·Ð°Ð´Ð°Ñ‡:
```bash
docker exec -it spark-master /opt/spark/bin/spark-shell --master spark://spark-master:7077
```

Ð’Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸ (Ñ€Ð°ÑÑ‡ÐµÑ‚ Ñ‡Ð¸ÑÐ»Ð° ÐŸÐ¸):
```scala
val count = sc.parallelize(1 to 100000).filter { _ =>
  val x = math.random
  val y = math.random
  x*x + y*y < 1
}.count()
println(s"Pi is roughly ${4.0 * count / 100000}")
```

### 4. Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ (ÐŸÑ€Ð¸Ð¼ÐµÑ€ NYC Taxi)
Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° (Parquet) Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ‡ÐµÑ€ÐµÐ· Spark SQL:

**Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…:**
```bash
# Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð» Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€
docker exec namenode curl -o /tmp/taxi.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet

# ÐšÐ»Ð°Ð´ÐµÐ¼ Ð² HDFS
docker exec namenode hdfs dfs -put /tmp/taxi.parquet /user/myself/
```

**ÐÐ½Ð°Ð»Ð¸Ð· (Ð² Spark Shell):**
```scala
val df = spark.read.parquet("hdfs://namenode:9000/user/myself/taxi.parquet")
df.createOrReplaceTempView("taxi")
spark.sql("SELECT passenger_count, count(*) FROM taxi GROUP BY 1").show()
```

## ðŸ“– ÐŸÐ»Ð°Ð½ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
Ð’ Ñ„Ð°Ð¹Ð»Ðµ `learning_plan.md` Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð¾Ð°Ð´Ð¼Ð°Ð¿ Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¹ Ð² ÑÑ‚Ð¾Ð¼ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ð¸.
